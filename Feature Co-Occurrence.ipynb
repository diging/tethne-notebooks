{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tethne.readers import wos\n",
    "datapath = '/Users/erickpeirson/Downloads/datasets/wos/genecol* OR common garden 1-500.txt'\n",
    "corpus = wos.read(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks of features based on co-occurrence\n",
    "\n",
    "The ``features`` module in the ``tethne.networks`` subpackage contains a few functions for generating networks of features based on co-occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tethne.networks import features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ``index_feature()`` to tokenize the abstract into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.index_feature('abstract', tokenize=lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all of the papers whose abstracts contain the word ``'arthropod'``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abstractTerms = corpus.features['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'WOS:000324532900018',\n",
       " u'WOS:000295132100020',\n",
       " u'WOS:000324408400012',\n",
       " u'WOS:000305342300080',\n",
       " u'WOS:000296938400006',\n",
       " u'WOS:000325555300008',\n",
       " u'WOS:000299058100016',\n",
       " u'WOS:000321823300005',\n",
       " u'WOS:000323699000010',\n",
       " u'WOS:000304300100004']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstractTerms.papers_containing('arthropod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``transform`` method allows us to transform the values from one featureset using a custom function. One popular transformation for wordcount data is the [term frequency * inverse document frequency (tf\\*idf)](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) transformation. tf\\*idf weights wordcounts for each document based on how frequent each word is in the rest of the corpus, and is supposed to bring to the foreground the words that are the most \"important\" for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "def tfidf(f, c, C, DC):\n",
    "    \"\"\"\n",
    "    Apply the term frequency * inverse document frequency transformation.\n",
    "    \"\"\"\n",
    "    tf = float(c)\n",
    "    idf = log(float(len(abstractTerms.features))/float(DC))\n",
    "    return tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.features['abstracts_tfidf'] = abstractTerms.transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can specify some other transformation by first defining a transformer function, and then passing it as an argument to ``transform``. A transformer function should accept the following parameters, and return a single numerical value (int or float).\n",
    "\n",
    "| Parameter | Description                                                       |\n",
    "| --------- | ----------------------------------------------------------------- |\n",
    "| ``f``     | Representation of the feature (e.g. string).                      |\n",
    "| ``v``     | Value of the feature in the document (e.g. frequency).            |\n",
    "| ``C``     | Value of the feature in the ``Corpus`` (e.g. global frequency).   |\n",
    "| ``DC``    | Number of documents in which the feature occcurs.                 |\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mytransformer(s, c, C, DC):\n",
    "    \"\"\"\n",
    "    Doubles the feature value and divides by the overall value in the Corpus.\n",
    "    \"\"\"\n",
    "    return c*2./(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass transformer function to ``transform`` as the first positional argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.features['abstracts_transformed'] = abstractTerms.transform(mytransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the impact on the value for 'arthropod' in one document, using the two transformations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:         4\n",
      "TF*IDF:         15.6240197324\n",
      "mytransformer:  0.307692307692\n"
     ]
    }
   ],
   "source": [
    "print 'Before: '.ljust(15), corpus.features['abstract'].features['WOS:000324532900018'].value('arthropod')\n",
    "print 'TF*IDF: '.ljust(15), corpus.features['abstracts_tfidf'].features['WOS:000324532900018'].value('arthropod')\n",
    "print 'mytransformer: '.ljust(15), corpus.features['abstracts_transformed'].features['WOS:000324532900018'].value('arthropod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use ``transform()`` to remove words from our ``FeatureSet``. For example, we can apply the NLTK stoplist and remove too-common or too-rare words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words()\n",
    "\n",
    "def apply_stoplist(f, v, c, dc):\n",
    "    if f in stoplist or dc > 50 or dc < 3:\n",
    "        return 0\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.features['abstracts_filtered'] = corpus.features['abstracts_tfidf'].transform(apply_stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:    20954\n",
      "After:     4557\n"
     ]
    }
   ],
   "source": [
    "print 'Before: '.ljust(10), len(corpus.features['abstracts_tfidf'].index)\n",
    "print 'After: '.ljust(10), len(corpus.features['abstracts_filtered'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``mutual_information`` function in the ``features`` module generates a network based on the [pointwise mutual information](http://en.wikipedia.org/wiki/Pointwise_mutual_information) of each pair of features in a featureset.\n",
    "\n",
    "The first argument is a list of Papers, just like most other network-building functions. The second argument is the featureset that we wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MI_graph = features.mutual_information(corpus, 'abstracts_filtered', min_weight=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the ratio of nodes to edges to get a sense of how to tune the ``min_weight`` parameter. If you have an extremely high number of edges for the number of nodes, then you should probably increase ``min_weight`` to obtain a more legible network. Depending on your field, you may have some guidance from theory as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This graph has 2271 nodes and 5595 edges\n"
     ]
    }
   ],
   "source": [
    "print 'This graph has {0} nodes and {1} edges'.format(MI_graph.order(), MI_graph.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll use the GraphML writer to generate a visualizable network file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tethne.writers import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mi_outpath = '/Users/erickpeirson/Projects/tethne-notebooks/output/mi_graph.graphml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph.to_graphml(MI_graph, mi_outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://devo-evo.lab.asu.edu/methods/tethne/images/pmi_99.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
